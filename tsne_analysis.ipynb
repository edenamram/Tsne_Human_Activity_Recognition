{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# t-SNE Analysis of Human Activity Recognition Dataset\n",
        "\n",
        "This notebook implements a comprehensive t-SNE analysis pipeline following the research methodology:\n",
        "\n",
        "1. **Dataset Selection**: Human Activity Recognition dataset (~7K samples)\n",
        "2. **Hyperparameter Testing**: perplexity=[5,30,50,100], early_exaggeration=[4,12]\n",
        "3. **PCA Preprocessing**: Dimensionality reduction to 50 components\n",
        "4. **t-SNE Implementation**: 2D embedding with various parameters\n",
        "5. **Quantitative Evaluation**: Continuity, Mean Local Error, Shepard Correlations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from scipy.stats import spearmanr\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TSNEAnalysis Class Definition\n",
        "class TSNEAnalysis:\n",
        "    def __init__(self, data_path):\n",
        "        \"\"\"Initialize the t-SNE analysis with dataset path\"\"\"\n",
        "        self.data_path = data_path\n",
        "        self.data = None\n",
        "        self.X = None\n",
        "        self.y = None\n",
        "        self.X_scaled = None\n",
        "        self.X_pca = None\n",
        "        self.results = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Loading and Exploration Method\n",
        "def load_and_explore_data(self):\n",
        "    \"\"\"Load data and perform EDA\"\"\"\n",
        "    print(\"=\"*50)\n",
        "    print(\"LOADING AND EXPLORING DATA\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Load data\n",
        "    self.data = pd.read_csv(self.data_path)\n",
        "    print(f\"Dataset shape: {self.data.shape}\")\n",
        "    print(f\"Dataset size: {self.data.size:,} total values\")\n",
        "    \n",
        "    # Basic info\n",
        "    print(\"\\nFirst few rows:\")\n",
        "    print(self.data.head())\n",
        "    \n",
        "    print(\"\\nDataset Info:\")\n",
        "    print(self.data.info())\n",
        "    \n",
        "    print(\"\\nColumn names:\")\n",
        "    print(self.data.columns.tolist())\n",
        "    \n",
        "    # Check for missing values\n",
        "    missing_values = self.data.isnull().sum()\n",
        "    print(f\"\\nMissing values: {missing_values.sum()}\")\n",
        "    if missing_values.sum() > 0:\n",
        "        print(missing_values[missing_values > 0])\n",
        "    \n",
        "    # Statistical summary\n",
        "    print(\"\\nStatistical Summary:\")\n",
        "    print(self.data.describe())\n",
        "    \n",
        "    return self.data\n",
        "\n",
        "# Add method to class\n",
        "TSNEAnalysis.load_and_explore_data = load_and_explore_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Preparation Method\n",
        "def prepare_data(self):\n",
        "    \"\"\"Prepare data for analysis - separate features and target\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"PREPARING DATA\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Assuming the last column is the target (common in many datasets)\n",
        "    # Let's identify categorical columns first\n",
        "    categorical_cols = self.data.select_dtypes(include=['object']).columns\n",
        "    numerical_cols = self.data.select_dtypes(include=[np.number]).columns\n",
        "    \n",
        "    print(f\"Categorical columns: {list(categorical_cols)}\")\n",
        "    print(f\"Numerical columns: {list(numerical_cols)}\")\n",
        "    \n",
        "    # If there are categorical columns, assume the last one is target\n",
        "    if len(categorical_cols) > 0:\n",
        "        target_col = categorical_cols[-1]\n",
        "        feature_cols = [col for col in self.data.columns if col != target_col]\n",
        "    else:\n",
        "        # If all numerical, assume last column is target\n",
        "        target_col = self.data.columns[-1]\n",
        "        feature_cols = self.data.columns[:-1].tolist()\n",
        "    \n",
        "    print(f\"Target column: {target_col}\")\n",
        "    print(f\"Number of features: {len(feature_cols)}\")\n",
        "    \n",
        "    # Separate features and target\n",
        "    self.X = self.data[feature_cols]\n",
        "    self.y = self.data[target_col]\n",
        "    \n",
        "    # Encode target if categorical\n",
        "    if self.y.dtype == 'object':\n",
        "        le = LabelEncoder()\n",
        "        self.y_encoded = le.fit_transform(self.y)\n",
        "        self.label_names = le.classes_\n",
        "        print(f\"Target classes: {self.label_names}\")\n",
        "        print(f\"Class distribution:\\n{pd.Series(self.y).value_counts()}\")\n",
        "    else:\n",
        "        self.y_encoded = self.y\n",
        "        self.label_names = None\n",
        "        \n",
        "    print(f\"Feature matrix shape: {self.X.shape}\")\n",
        "    print(f\"Target vector shape: {self.y_encoded.shape}\")\n",
        "    \n",
        "    return self.X, self.y_encoded\n",
        "\n",
        "# Add method to class\n",
        "TSNEAnalysis.prepare_data = prepare_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function for counting high correlations\n",
        "def _count_high_correlations(self):\n",
        "    \"\"\"Count highly correlated feature pairs\"\"\"\n",
        "    if self.X.shape[1] > 100:  # Sample for large datasets\n",
        "        sample_X = self.X.sample(n=100, axis=1)\n",
        "    else:\n",
        "        sample_X = self.X\n",
        "        \n",
        "    corr_matrix = sample_X.corr()\n",
        "    high_corr = (corr_matrix.abs() > 0.9) & (corr_matrix.abs() < 1.0)\n",
        "    return high_corr.sum().sum() // 2  # Divide by 2 to avoid double counting\n",
        "\n",
        "# Add helper method to class\n",
        "TSNEAnalysis._count_high_correlations = _count_high_correlations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exploratory Data Analysis Method\n",
        "def perform_eda(self):\n",
        "    \"\"\"Perform Exploratory Data Analysis\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"EXPLORATORY DATA ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Create figure for EDA plots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Exploratory Data Analysis', fontsize=16)\n",
        "    \n",
        "    # 1. Target distribution\n",
        "    if self.label_names is not None:\n",
        "        labels = self.label_names\n",
        "        counts = pd.Series(self.y).value_counts()\n",
        "    else:\n",
        "        labels = pd.Series(self.y_encoded).value_counts().index\n",
        "        counts = pd.Series(self.y_encoded).value_counts()\n",
        "        \n",
        "    axes[0,0].pie(counts.values, labels=labels, autopct='%1.1f%%')\n",
        "    axes[0,0].set_title('Target Distribution')\n",
        "    \n",
        "    # 2. Feature correlation heatmap (sample of features if too many)\n",
        "    if self.X.shape[1] > 20:\n",
        "        sample_features = self.X.iloc[:, :20]\n",
        "        axes[0,1].set_title('Correlation Heatmap (First 20 features)')\n",
        "    else:\n",
        "        sample_features = self.X\n",
        "        axes[0,1].set_title('Feature Correlation Heatmap')\n",
        "        \n",
        "    corr_matrix = sample_features.corr()\n",
        "    sns.heatmap(corr_matrix, ax=axes[0,1], cmap='coolwarm', center=0, \n",
        "               square=True, cbar_kws={\"shrink\": .8})\n",
        "    \n",
        "    # 3. Feature variance\n",
        "    feature_vars = self.X.var().sort_values(ascending=False)\n",
        "    top_vars = feature_vars.head(20)\n",
        "    axes[1,0].bar(range(len(top_vars)), top_vars.values)\n",
        "    axes[1,0].set_title('Top 20 Features by Variance')\n",
        "    axes[1,0].set_xlabel('Feature Index')\n",
        "    axes[1,0].set_ylabel('Variance')\n",
        "    \n",
        "    # 4. Sample feature distributions\n",
        "    if self.X.shape[1] >= 4:\n",
        "        sample_cols = self.X.columns[:4]\n",
        "    else:\n",
        "        sample_cols = self.X.columns\n",
        "        \n",
        "    for i, col in enumerate(sample_cols):\n",
        "        if i < 4:\n",
        "            row, col_idx = (1, 1) if i < 2 else (1, 1)\n",
        "            if i == 0:\n",
        "                axes[1,1].hist(self.X.iloc[:, i], bins=30, alpha=0.7, label=f'Feature {i+1}')\n",
        "            elif i == 1:\n",
        "                axes[1,1].hist(self.X.iloc[:, i], bins=30, alpha=0.7, label=f'Feature {i+1}')\n",
        "                \n",
        "    axes[1,1].set_title('Sample Feature Distributions')\n",
        "    axes[1,1].legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('eda_plots.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # Print feature statistics\n",
        "    print(f\"\\nFeature Statistics:\")\n",
        "    print(f\"Number of features: {self.X.shape[1]}\")\n",
        "    print(f\"Features with zero variance: {(self.X.var() == 0).sum()}\")\n",
        "    print(f\"Highly correlated feature pairs (>0.9): {self._count_high_correlations()}\")\n",
        "\n",
        "# Add method to class\n",
        "TSNEAnalysis.perform_eda = perform_eda\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Normalization Method\n",
        "def normalize_data(self):\n",
        "    \"\"\"Normalize the features\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"DATA NORMALIZATION\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    scaler = StandardScaler()\n",
        "    self.X_scaled = scaler.fit_transform(self.X)\n",
        "    \n",
        "    print(\"Data normalized using StandardScaler (mean=0, std=1)\")\n",
        "    print(f\"Original data shape: {self.X.shape}\")\n",
        "    print(f\"Normalized data shape: {self.X_scaled.shape}\")\n",
        "    print(f\"Normalized data mean: {np.mean(self.X_scaled):.6f}\")\n",
        "    print(f\"Normalized data std: {np.std(self.X_scaled):.6f}\")\n",
        "    \n",
        "    return self.X_scaled\n",
        "\n",
        "# Add method to class\n",
        "TSNEAnalysis.normalize_data = normalize_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PCA Dimensionality Reduction Method\n",
        "def apply_pca(self, n_components=50):\n",
        "    \"\"\"Apply PCA for dimensionality reduction\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"PCA DIMENSIONALITY REDUCTION\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Adjust n_components if necessary\n",
        "    max_components = min(self.X_scaled.shape[0], self.X_scaled.shape[1])\n",
        "    n_components = min(n_components, max_components)\n",
        "    \n",
        "    pca = PCA(n_components=n_components)\n",
        "    self.X_pca = pca.fit_transform(self.X_scaled)\n",
        "    \n",
        "    print(f\"PCA applied: {self.X_scaled.shape[1]} -> {n_components} dimensions\")\n",
        "    print(f\"Explained variance ratio: {pca.explained_variance_ratio_[:10]}\")\n",
        "    print(f\"Cumulative explained variance: {pca.explained_variance_ratio_.cumsum()[:10]}\")\n",
        "    print(f\"Total explained variance: {pca.explained_variance_ratio_.sum():.4f}\")\n",
        "    \n",
        "    # Plot explained variance\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(1, min(21, len(pca.explained_variance_ratio_) + 1)), \n",
        "            pca.explained_variance_ratio_[:20], 'bo-')\n",
        "    plt.title('Explained Variance by Component')\n",
        "    plt.xlabel('Principal Component')\n",
        "    plt.ylabel('Explained Variance Ratio')\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
        "            pca.explained_variance_ratio_.cumsum(), 'ro-')\n",
        "    plt.title('Cumulative Explained Variance')\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('Cumulative Explained Variance')\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('pca_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    return self.X_pca\n",
        "\n",
        "# Add method to class\n",
        "TSNEAnalysis.apply_pca = apply_pca\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# t-SNE Experiments Method\n",
        "def run_tsne_experiments(self, perplexities=[5, 30, 50, 100], \n",
        "                       early_exaggerations=[4, 12], use_pca=True):\n",
        "    \"\"\"Run t-SNE with various hyperparameters\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"T-SNE EXPERIMENTS\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Choose input data\n",
        "    if use_pca and self.X_pca is not None:\n",
        "        input_data = self.X_pca\n",
        "        print(f\"Using PCA-reduced data: {input_data.shape}\")\n",
        "    else:\n",
        "        input_data = self.X_scaled\n",
        "        print(f\"Using normalized data: {input_data.shape}\")\n",
        "    \n",
        "    # Limit data size if too large (t-SNE is computationally expensive)\n",
        "    if input_data.shape[0] > 5000:\n",
        "        indices = np.random.choice(input_data.shape[0], 5000, replace=False)\n",
        "        input_data = input_data[indices]\n",
        "        y_sample = self.y_encoded[indices] if hasattr(self, 'y_encoded') else self.y_encoded[indices]\n",
        "        self._tsne_indices = indices  # Store indices for evaluation\n",
        "        print(f\"Sampled data for t-SNE: {input_data.shape}\")\n",
        "    else:\n",
        "        y_sample = self.y_encoded\n",
        "        self._tsne_indices = np.arange(input_data.shape[0])  # Store all indices\n",
        "    \n",
        "    # Run experiments\n",
        "    fig, axes = plt.subplots(len(early_exaggerations), len(perplexities), \n",
        "                           figsize=(5*len(perplexities), 5*len(early_exaggerations)))\n",
        "    \n",
        "    if len(early_exaggerations) == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "    if len(perplexities) == 1:\n",
        "        axes = axes.reshape(-1, 1)\n",
        "    \n",
        "    for i, early_exag in enumerate(early_exaggerations):\n",
        "        for j, perplexity in enumerate(perplexities):\n",
        "            print(f\"\\nRunning t-SNE: perplexity={perplexity}, early_exaggeration={early_exag}\")\n",
        "            \n",
        "            tsne = TSNE(n_components=2, perplexity=perplexity, \n",
        "                       early_exaggeration=early_exag, random_state=42,\n",
        "                       max_iter=1000, verbose=0)\n",
        "            \n",
        "            tsne_result = tsne.fit_transform(input_data)\n",
        "            \n",
        "            # Store results\n",
        "            self.results[f'perp_{perplexity}_exag_{early_exag}'] = {\n",
        "                'embedding': tsne_result,\n",
        "                'perplexity': perplexity,\n",
        "                'early_exaggeration': early_exag,\n",
        "                'kl_divergence': tsne.kl_divergence_\n",
        "            }\n",
        "            \n",
        "            # Plot\n",
        "            scatter = axes[i, j].scatter(tsne_result[:, 0], tsne_result[:, 1], \n",
        "                                       c=y_sample, cmap='tab10', alpha=0.7, s=10)\n",
        "            axes[i, j].set_title(f'Perplexity={perplexity}, Early Exag={early_exag}')\n",
        "            axes[i, j].set_xlabel('t-SNE 1')\n",
        "            axes[i, j].set_ylabel('t-SNE 2')\n",
        "            \n",
        "            print(f\"KL divergence: {tsne.kl_divergence_:.4f}\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('tsne_experiments.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    return self.results\n",
        "\n",
        "# Add method to class\n",
        "TSNEAnalysis.run_tsne_experiments = run_tsne_experiments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper Methods for Evaluation Metrics\n",
        "def _calculate_continuity(self, original_distances, embedding_distances, k=7):\n",
        "    \"\"\"Calculate continuity metric\"\"\"\n",
        "    n = original_distances.shape[0]\n",
        "    continuity_scores = []\n",
        "    \n",
        "    for i in range(n):\n",
        "        # Find k nearest neighbors in original space\n",
        "        orig_neighbors = np.argsort(original_distances[i])[1:k+1]\n",
        "        \n",
        "        # Find k nearest neighbors in embedding space\n",
        "        embed_neighbors = np.argsort(embedding_distances[i])[1:k+1]\n",
        "        \n",
        "        # Calculate overlap\n",
        "        overlap = len(set(orig_neighbors) & set(embed_neighbors))\n",
        "        continuity_scores.append(overlap / k)\n",
        "    \n",
        "    return np.mean(continuity_scores)\n",
        "\n",
        "def _calculate_local_error(self, original_distances, embedding_distances, k=7):\n",
        "    \"\"\"Calculate mean local error\"\"\"\n",
        "    n = original_distances.shape[0]\n",
        "    local_errors = []\n",
        "    \n",
        "    for i in range(n):\n",
        "        # Find k nearest neighbors in original space\n",
        "        orig_neighbors = np.argsort(original_distances[i])[1:k+1]\n",
        "        \n",
        "        # Calculate rank errors in embedding space\n",
        "        embed_ranks = np.argsort(np.argsort(embedding_distances[i]))\n",
        "        \n",
        "        rank_errors = []\n",
        "        for neighbor in orig_neighbors:\n",
        "            original_rank = np.where(np.argsort(original_distances[i]) == neighbor)[0][0]\n",
        "            embedding_rank = embed_ranks[neighbor]\n",
        "            rank_errors.append(abs(original_rank - embedding_rank))\n",
        "        \n",
        "        local_errors.append(np.mean(rank_errors))\n",
        "    \n",
        "    return np.mean(local_errors)\n",
        "\n",
        "# Add helper methods to class\n",
        "TSNEAnalysis._calculate_continuity = _calculate_continuity\n",
        "TSNEAnalysis._calculate_local_error = _calculate_local_error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation Summary Plotting Method\n",
        "def _plot_evaluation_summary(self, evaluation_results):\n",
        "    \"\"\"Plot evaluation metrics summary\"\"\"\n",
        "    metrics = ['shepard_correlation', 'continuity', 'local_error', 'kl_divergence']\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i, metric in enumerate(metrics):\n",
        "        values = [result[metric] for result in evaluation_results.values()]\n",
        "        labels = list(evaluation_results.keys())\n",
        "        \n",
        "        axes[i].bar(range(len(values)), values)\n",
        "        axes[i].set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
        "        axes[i].set_xticks(range(len(labels)))\n",
        "        axes[i].set_xticklabels(labels, rotation=45, ha='right')\n",
        "        \n",
        "        # Add value labels on bars\n",
        "        for j, v in enumerate(values):\n",
        "            axes[i].text(j, v, f'{v:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('evaluation_summary.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Add method to class\n",
        "TSNEAnalysis._plot_evaluation_summary = _plot_evaluation_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding Evaluation Method\n",
        "def evaluate_embeddings(self, use_pca=True):\n",
        "    \"\"\"Evaluate embeddings using various metrics\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"EMBEDDING EVALUATION\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Choose original data\n",
        "    if use_pca and self.X_pca is not None:\n",
        "        original_data = self.X_pca\n",
        "    else:\n",
        "        original_data = self.X_scaled\n",
        "    \n",
        "    # Use consistent sampling for both original data and embeddings\n",
        "    n_eval_samples = min(1000, original_data.shape[0])\n",
        "    \n",
        "    # Get indices that were used for t-SNE (if sampling was done)\n",
        "    if hasattr(self, '_tsne_indices'):\n",
        "        # Use the same indices that were used for t-SNE\n",
        "        available_indices = self._tsne_indices[:n_eval_samples]\n",
        "        original_sample = original_data[available_indices]\n",
        "    else:\n",
        "        # Create new sample\n",
        "        indices = np.random.choice(original_data.shape[0], n_eval_samples, replace=False)\n",
        "        original_sample = original_data[indices]\n",
        "    \n",
        "    print(f\"Evaluating with {original_sample.shape[0]} samples\")\n",
        "    \n",
        "    # Compute original distances\n",
        "    original_distances = pairwise_distances(original_sample)\n",
        "    \n",
        "    evaluation_results = {}\n",
        "    \n",
        "    for key, result in self.results.items():\n",
        "        embedding = result['embedding']\n",
        "        \n",
        "        # Use the same number of samples for embedding\n",
        "        embedding_sample = embedding[:n_eval_samples]\n",
        "        \n",
        "        # Compute embedding distances\n",
        "        embedding_distances = pairwise_distances(embedding_sample)\n",
        "        \n",
        "        # Flatten distance matrices for correlation\n",
        "        orig_dist_flat = original_distances[np.triu_indices_from(original_distances, k=1)]\n",
        "        embed_dist_flat = embedding_distances[np.triu_indices_from(embedding_distances, k=1)]\n",
        "        \n",
        "        # Calculate metrics\n",
        "        # 1. Spearman correlation (Shepard correlation)\n",
        "        shepard_corr, _ = spearmanr(orig_dist_flat, embed_dist_flat)\n",
        "        \n",
        "        # 2. Continuity (preservation of neighborhoods)\n",
        "        continuity = self._calculate_continuity(original_distances, embedding_distances)\n",
        "        \n",
        "        # 3. Mean Local Error\n",
        "        local_error = self._calculate_local_error(original_distances, embedding_distances)\n",
        "        \n",
        "        evaluation_results[key] = {\n",
        "            'shepard_correlation': shepard_corr,\n",
        "            'continuity': continuity,\n",
        "            'local_error': local_error,\n",
        "            'kl_divergence': result['kl_divergence']\n",
        "        }\n",
        "        \n",
        "        print(f\"\\n{key}:\")\n",
        "        print(f\"  Shepard Correlation: {shepard_corr:.4f}\")\n",
        "        print(f\"  Continuity: {continuity:.4f}\")\n",
        "        print(f\"  Local Error: {local_error:.4f}\")\n",
        "        print(f\"  KL Divergence: {result['kl_divergence']:.4f}\")\n",
        "    \n",
        "    # Create evaluation summary plot\n",
        "    self._plot_evaluation_summary(evaluation_results)\n",
        "    \n",
        "    return evaluation_results\n",
        "\n",
        "# Add method to class\n",
        "TSNEAnalysis.evaluate_embeddings = evaluate_embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Best Configuration Finding Method\n",
        "def _find_best_configuration(self):\n",
        "    \"\"\"Find best t-SNE configuration based on multiple metrics\"\"\"\n",
        "    scores = {}\n",
        "    \n",
        "    for key, metrics in self.evaluation_results.items():\n",
        "        # Normalize metrics (higher is better for shepard_correlation and continuity)\n",
        "        # Lower is better for local_error and kl_divergence\n",
        "        score = (metrics['shepard_correlation'] + metrics['continuity'] - \n",
        "                metrics['local_error']/10 - metrics['kl_divergence']/100)\n",
        "        scores[key] = score\n",
        "    \n",
        "    return max(scores.items(), key=lambda x: x[1])\n",
        "\n",
        "# Add method to class\n",
        "TSNEAnalysis._find_best_configuration = _find_best_configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Report Generation Method\n",
        "def generate_report(self):\n",
        "    \"\"\"Generate a comprehensive analysis report\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ANALYSIS REPORT\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    print(f\"Dataset: {self.data_path}\")\n",
        "    print(f\"Original dimensions: {self.X.shape}\")\n",
        "    print(f\"Number of classes: {len(np.unique(self.y_encoded))}\")\n",
        "    \n",
        "    if hasattr(self, 'X_pca') and self.X_pca is not None:\n",
        "        print(f\"PCA dimensions: {self.X_pca.shape}\")\n",
        "    \n",
        "    print(f\"\\nt-SNE Experiments conducted: {len(self.results)}\")\n",
        "    \n",
        "    # Find best configuration based on multiple metrics\n",
        "    if hasattr(self, 'evaluation_results'):\n",
        "        best_config = self._find_best_configuration()\n",
        "        print(f\"\\nBest configuration based on combined metrics: {best_config}\")\n",
        "    \n",
        "    print(\"\\nFiles generated:\")\n",
        "    print(\"- eda_plots.png: Exploratory data analysis\")\n",
        "    print(\"- pca_analysis.png: PCA variance analysis\")\n",
        "    print(\"- tsne_experiments.png: t-SNE results with different parameters\")\n",
        "    print(\"- evaluation_summary.png: Evaluation metrics comparison\")\n",
        "\n",
        "# Add method to class\n",
        "TSNEAnalysis.generate_report = generate_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete Analysis Pipeline Method\n",
        "def run_complete_analysis(self):\n",
        "    \"\"\"Run the complete t-SNE analysis pipeline\"\"\"\n",
        "    print(\"Starting Complete t-SNE Analysis Pipeline\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Step 1: Load and explore data\n",
        "    self.load_and_explore_data()\n",
        "    \n",
        "    # Step 2: Prepare data\n",
        "    self.prepare_data()\n",
        "    \n",
        "    # Step 3: EDA\n",
        "    self.perform_eda()\n",
        "    \n",
        "    # Step 4: Normalization\n",
        "    self.normalize_data()\n",
        "    \n",
        "    # Step 5: PCA (optional)\n",
        "    self.apply_pca(n_components=50)\n",
        "    \n",
        "    # Step 6: t-SNE experiments\n",
        "    self.run_tsne_experiments(\n",
        "        perplexities=[5, 30, 50, 100],\n",
        "        early_exaggerations=[4, 12],\n",
        "        use_pca=True\n",
        "    )\n",
        "    \n",
        "    # Step 7: Evaluation\n",
        "    self.evaluation_results = self.evaluate_embeddings(use_pca=True)\n",
        "    \n",
        "    # Step 8: Generate report\n",
        "    self.generate_report()\n",
        "    \n",
        "    print(\"\\nAnalysis completed successfully!\")\n",
        "    return self\n",
        "\n",
        "# Add method to class\n",
        "TSNEAnalysis.run_complete_analysis = run_complete_analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Initialize Analysis\n",
        "\n",
        "Create the analysis object and run the complete pipeline. You can run this cell to execute the full analysis, or run individual cells below for step-by-step analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Analysis and Run Complete Pipeline\n",
        "analysis = TSNEAnalysis(\"train.csv\")\n",
        "\n",
        "# Run complete analysis\n",
        "analysis.run_complete_analysis()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Alternative: Step-by-Step Analysis\n",
        "\n",
        "If you prefer to run the analysis step by step, you can use the cells below instead of the complete pipeline above. This allows for more interactive exploration and customization of parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Initialize and Load Data\n",
        "# analysis = TSNEAnalysis(\"train.csv\")\n",
        "# analysis.load_and_explore_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Prepare Data\n",
        "# analysis.prepare_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Exploratory Data Analysis\n",
        "# analysis.perform_eda()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Data Normalization\n",
        "# analysis.normalize_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: PCA Dimensionality Reduction\n",
        "# analysis.apply_pca(n_components=50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: t-SNE Experiments\n",
        "# analysis.run_tsne_experiments(\n",
        "#     perplexities=[5, 30, 50, 100],\n",
        "#     early_exaggerations=[4, 12],\n",
        "#     use_pca=True\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7: Evaluate Embeddings\n",
        "# analysis.evaluation_results = analysis.evaluate_embeddings(use_pca=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 8: Generate Report\n",
        "# analysis.generate_report()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Notes\n",
        "\n",
        "### Usage Options:\n",
        "\n",
        "1. **Complete Pipeline**: Run cell 17 to execute the entire analysis at once\n",
        "2. **Step-by-Step**: Uncomment and run cells 19-26 individually for interactive analysis\n",
        "\n",
        "### Customization:\n",
        "\n",
        "- Modify `perplexities` and `early_exaggerations` parameters in the t-SNE experiments\n",
        "- Adjust `n_components` for PCA to test different dimensionalities\n",
        "- Change evaluation sample size in `evaluate_embeddings()` method\n",
        "\n",
        "### Generated Files:\n",
        "\n",
        "- `eda_plots.png`: Exploratory data analysis visualizations\n",
        "- `pca_analysis.png`: PCA variance explanation plots\n",
        "- `tsne_experiments.png`: t-SNE results with all parameter combinations\n",
        "- `evaluation_summary.png`: Quantitative evaluation metrics comparison\n",
        "\n",
        "### Evaluation Metrics:\n",
        "\n",
        "- **Shepard Correlation**: Measures preservation of pairwise distances (higher is better)\n",
        "- **Continuity**: Fraction of k-nearest neighbors preserved (higher is better)\n",
        "- **Local Error**: Average ranking error in neighborhoods (lower is better)\n",
        "- **KL Divergence**: t-SNE optimization objective (lower is better)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
